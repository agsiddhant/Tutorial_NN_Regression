{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T12:59:09.188116Z",
     "iopub.status.busy": "2021-11-24T12:59:09.187729Z",
     "iopub.status.idle": "2021-11-24T12:59:13.131688Z",
     "shell.execute_reply": "2021-11-24T12:59:13.131273Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T12:59:13.134666Z",
     "iopub.status.busy": "2021-11-24T12:59:13.134253Z",
     "iopub.status.idle": "2021-11-24T12:59:13.160230Z",
     "shell.execute_reply": "2021-11-24T12:59:13.160555Z"
    }
   },
   "outputs": [],
   "source": [
    "pathSave = \"/plp_user/agar_si/gaia/python/Tutorial_NN_Regression/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This tutorial compares linear regression with feed-forward neural networks for \"low-dimensional\" surrogate modeling. We use a dataset of 2D computational fluid dynamic simulations of the mantle (highly viscous flow) for a Mars-like planet. The regression task is to predict the 1D temperature profile from five simulation parameters at any point in the 4.5 Gyr evolution. By the end of this tutorial, you will know how to build, train and load a feed-forward neural network using [Keras](https://keras.io/). \n",
    "\n",
    "<img src=\"Diagrams-Concept-1D.svg\" alt=\"Drawing\" style=\"width: 720px;\"/>\n",
    "\n",
    "### Tutorial structure\n",
    "There are three modules:\n",
    "> **Module 1** Data processing <br> \n",
    "> **Module 2** Training regression algorithms <br> \n",
    "> **Module 3** Testing accuracy of trained algorithms <br> \n",
    "\n",
    "### To dos\n",
    "Headings marked in <span style='color:blue'> blue </span> indicate that the following cell needs to be completed. Each line of code that needs to be added is further indicated by <span style='color:gray'> \"### TO DO\" </span>. You will need to replace all instances where <span style='color:violet'> --- </span> appears. We will go through all the cells together, but feel free to check the \"Regression_Tutorial_sol.ipynb\" notebook for example solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1/3: Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the number of simulations to use for training\n",
    "\n",
    "The dataset contains a total of 10525 simulations for a Mars-like planet carried out in a quarter-cylindrical 2D geometry. 1D temperature profiles and the input parameters for the simulations have already been extracted and stored as arrays in Data/processed/\n",
    "\n",
    "The following numbers of numTrainingSims can be specified: 100, 1000, 3000, 6000, 9877. We specify 1000 for a good balance between speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T12:59:13.162999Z",
     "iopub.status.busy": "2021-11-24T12:59:13.162642Z",
     "iopub.status.idle": "2021-11-24T12:59:13.188418Z",
     "shell.execute_reply": "2021-11-24T12:59:13.188709Z"
    }
   },
   "outputs": [],
   "source": [
    "numTrainingSims = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "Here we load the following data: <br>\n",
    "> **training:** used to train the regression algorithms <br>\n",
    "> **cross-validation:** used to monitor how the algorithm is performing on unseen data during training. It can also be used to search through different architectures. <br>\n",
    "> **test:** use to evaluate the accuracy of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T12:59:13.193082Z",
     "iopub.status.busy": "2021-11-24T12:59:13.192471Z",
     "iopub.status.idle": "2021-11-24T12:59:13.242195Z",
     "shell.execute_reply": "2021-11-24T12:59:13.241882Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(pathSave + \"Data/processed/x_train_\" + str(numTrainingSims) + \"Sims.txt\", \"rb\") as f:\n",
    "    x_train = pickle.load(f)\n",
    "with open(pathSave + \"Data/processed/y_train_\" + str(numTrainingSims) + \"Sims.txt\", \"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open(pathSave + \"Data/processed/x_cv.txt\", \"rb\") as f:\n",
    "    x_cv = pickle.load(f)\n",
    "with open(pathSave + \"Data/processed/y_cv.txt\", \"rb\") as f:\n",
    "    y_cv = pickle.load(f)\n",
    "    \n",
    "with open(pathSave + \"Data/processed/x_test.txt\", \"rb\") as f:\n",
    "    x_test = pickle.load(f)\n",
    "with open(pathSave + \"Data/processed/y_test.txt\", \"rb\") as f:\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'> Print the shape of the data </span>\n",
    "Let's print the shape of the arrays above using *.shape*\n",
    "\n",
    "Both, the input and output arrays generally have the shape of (number_examples, dimension of each example). Since, each simulation is broken into time-steps, each example for us is then 1 time-step of a simulation.<br>\n",
    "> **input:** Each example consists of 6 parameters: Time during the evolution, rayleigh number, activation energy of the diffusion creep, activation volume of the diffusion creep, crustal enrichment factor for radiogenic elements and initial mantle temperature. <br>\n",
    "> **output:** 1D temperature profiles (302-dimensional) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T12:59:13.245272Z",
     "iopub.status.busy": "2021-11-24T12:59:13.244833Z",
     "iopub.status.idle": "2021-11-24T12:59:13.272035Z",
     "shell.execute_reply": "2021-11-24T12:59:13.272355Z"
    }
   },
   "outputs": [],
   "source": [
    "### TO DO\n",
    "print(\"Training data        : \" + str(---) + \" \" + str(---))\n",
    "\n",
    "### TO DO\n",
    "print(\"cross-validation data: \" + str(---)    + \" \" + str(---))\n",
    "\n",
    "### TO DO\n",
    "print(\"Test data            : \" + str(---)    + \" \" + str(---))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "Let's plot both **x_train** and **y_train** to get an idea of if the parameter space is well covered and what scales the variables exist on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T12:59:13.277632Z",
     "iopub.status.busy": "2021-11-24T12:59:13.277076Z",
     "iopub.status.idle": "2021-11-24T12:59:14.899553Z",
     "shell.execute_reply": "2021-11-24T12:59:14.899889Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_parameters():\n",
    "    parameter_vec = [\"$t$\", \"$log_{10}Ra$\", \"$E$\", \"$V$\", \"$\\Lambda$\", \"$T_{ini}$\"]\n",
    "    fig = plt.figure(figsize=(21,3), dpi=320)\n",
    "    for i in range(len(parameter_vec)):\n",
    "        ax = fig.add_subplot(1,len(parameter_vec),i+1)\n",
    "        ax.plot(x_train[::100,i].flatten(),\"b.\")  # plotting only every 100th training example\n",
    "        ax.set_ylabel(parameter_vec[i])\n",
    "        ax.set_xlabel(\"Training example\")\n",
    "    plt.tight_layout(w_pad=0.001)\n",
    "    plt.show()\n",
    "    \n",
    "visualize_parameters()\n",
    "\n",
    "rProf = np.linspace(0.33, 1.33, 302)\n",
    "fig = plt.figure(figsize=(21,3), dpi=320)\n",
    "for i in range(6):           # plotting 6 example temperature profiles\n",
    "    ax = fig.add_subplot(1,6,i+1)\n",
    "    ax.plot(y_train[int(i*210),:],rProf,\"b-\")\n",
    "    ax.set_ylabel(\"Radius\")\n",
    "    ax.set_xlabel(\"Temperature\")\n",
    "plt.tight_layout(w_pad=0.001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'> Define a function to scale data </span>\n",
    "For gradient descent to work well, it is always a good idea to have different features on the same scale. \n",
    "So, let's define a function that scales an array between 0 and 1 using: <br>\n",
    "> x_scaled = (x_original - min)/(max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T12:59:14.903591Z",
     "iopub.status.busy": "2021-11-24T12:59:14.903215Z",
     "iopub.status.idle": "2021-11-24T12:59:14.934906Z",
     "shell.execute_reply": "2021-11-24T12:59:14.934590Z"
    }
   },
   "outputs": [],
   "source": [
    "### TO DO\n",
    "def non_dimensionalize(_x,_xmin,_xmax):\n",
    "    ### TO DO\n",
    "    _x -= ---\n",
    "    \n",
    "    ### TO DO\n",
    "    _x /= ---\n",
    "    return _x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'> Scale the data </span>\n",
    "\n",
    "Let's use the previously defined function *non_dimensionalize()* to scale **x_train**, **x_cv** and **x_test**. <br>\n",
    "\n",
    "**y_train**, **y_cv** and **y_test** are already roughly between 0 and 1, so we can skip scaling it. <br>\n",
    "The minimum and maximum values of the parameters are loaded and printed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T12:59:14.939660Z",
     "iopub.status.busy": "2021-11-24T12:59:14.939285Z",
     "iopub.status.idle": "2021-11-24T12:59:15.852133Z",
     "shell.execute_reply": "2021-11-24T12:59:15.852468Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(pathSave + \"Data/meta_data_dict.txt\", \"rb\") as f:\n",
    "    x_meta = pickle.load(f)\n",
    "    \n",
    "x_min = x_meta[\"xmin\"]\n",
    "x_max = x_meta[\"xmax\"]\n",
    "\n",
    "print(\"Min value of parameters: \" + str(x_min))\n",
    "print(\"Max value of parameters: \" + str(x_max))\n",
    "\n",
    "### TO DO\n",
    "for i in range(x_train.shape[1]):\n",
    "    ### TO DO\n",
    "    x_train[:,i] =  ---\n",
    "    \n",
    "    ### TO DO\n",
    "    x_cv[:,i]    =  ---\n",
    "    \n",
    "    ### TO DO\n",
    "    x_test[:,i]  =  ---\n",
    "    \n",
    "print()\n",
    "print(\"Visualizing the scaled data\")\n",
    "visualize_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2/3: Training regression algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'> Define a function to build a Keras network </span>\n",
    "\n",
    "Let's define a function that defines and returns an untrained Keras network. It has three main components: <br>\n",
    "1. **Input:** Define the shape of the input (only the second, *never* the batch shape) using *tf.keras.layers.Input(shape=(second dimension of x,))*\n",
    "2. **Hidden layer:** Define the hidden layers for a \"deep\" neural network using *tf.keras.layers.Dense(units, activation=\"tanh\")*. Other activations are also available such as \"relu\", \"selu\", etc.\n",
    "3. **Output:** Define the output layer also using a *tf.keras.layers.Dense(second dimension of y)*, but with no activation. The shape of the last layer much match the shape of the output (the shape of the temperature profile in our case).\n",
    "\n",
    "<img src=\"Diagrams-NN-Specific.svg\" alt=\"Drawing\" style=\"width: 720px;\"/>\n",
    "\n",
    "We can build this function to flexibly build a network with any number and size of hidden layers through the input argument *numNeurons=[]*. No hidden layers (i.e. *numNeurons=[]*) reduces the model to a linear regression model of the form (y = mx + b). By specifying, for example, *numNeurons=[300, 300]*, 2 hidden layers with 300 units are specified.\n",
    "\n",
    "We also compile the network within this function and specify the loss function (mean squared error; see https://keras.io/api/losses/ for more) and the optimizer to train the network weights (Adam; see https://keras.io/api/optimizers/ for more). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T12:59:15.857034Z",
     "iopub.status.busy": "2021-11-24T12:59:15.856674Z",
     "iopub.status.idle": "2021-11-24T12:59:15.896989Z",
     "shell.execute_reply": "2021-11-24T12:59:15.896681Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_network(numNeurons=[]):\n",
    "\n",
    "    ### TO DO\n",
    "    x = ---\n",
    "    h = x\n",
    "\n",
    "    for units in numNeurons:\n",
    "        ### TO DO\n",
    "        h = ---\n",
    "\n",
    "    ### TO DO\n",
    "    y = ---\n",
    "\n",
    "    network = tf.keras.models.Model(inputs=x, outputs=y)\n",
    "    network.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"mse\")             \n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a linear regression model\n",
    "\n",
    "Let's use *build_network()* to define a linear regression model (no hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T12:59:15.899914Z",
     "iopub.status.busy": "2021-11-24T12:59:15.899546Z",
     "iopub.status.idle": "2021-11-24T12:59:16.546186Z",
     "shell.execute_reply": "2021-11-24T12:59:16.545783Z"
    }
   },
   "outputs": [],
   "source": [
    "numNeurons_linear = []\n",
    "NN_linear = build_network(numNeurons_linear)\n",
    "NN_linear.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the linear regression model\n",
    "\n",
    "Now let's train *NN_linear*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T12:59:16.552124Z",
     "iopub.status.busy": "2021-11-24T12:59:16.551771Z",
     "iopub.status.idle": "2021-11-24T13:00:13.441931Z",
     "shell.execute_reply": "2021-11-24T13:00:13.442319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a session\n",
    "session = InteractiveSession()\n",
    "\n",
    "# The name under which the trained network is saved\n",
    "nameNetwork_linear = str(numNeurons_linear) + \"_\" + str(numTrainingSims)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def scheduler(epoch):\n",
    "    _learnRate = 1e-3\n",
    "    if epoch < 5:\n",
    "        return _learnRate\n",
    "    elif epoch >= 5 and epoch < 20:\n",
    "        return _learnRate/10\n",
    "    else:\n",
    "        return _learnRate/100\n",
    "\n",
    "# Define the callbacks\n",
    "lr_scheduler= tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "csvlogger   = tf.keras.callbacks.CSVLogger(pathSave+\"TrainedNetworks/\"+ nameNetwork_linear+\".txt\",separator=\",\",append=False)\n",
    "cp          = tf.keras.callbacks.ModelCheckpoint(pathSave+\"TrainedNetworks/\"+nameNetwork_linear+\".hdf5\",save_best_only=True)\n",
    "\n",
    "# Fit the network\n",
    "ae = NN_linear.fit(x_train,y_train,\n",
    "            verbose=1,\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            callbacks=[cp, lr_scheduler, csvlogger],\n",
    "            validation_data=(x_cv, y_cv),\n",
    "            shuffle=True) \n",
    "\n",
    "# Close the session\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'> Set up a deep neural network</span>\n",
    "\n",
    "Let's repeat setting up the network, but this time with some hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T13:00:13.446502Z",
     "iopub.status.busy": "2021-11-24T13:00:13.446130Z",
     "iopub.status.idle": "2021-11-24T13:00:14.354540Z",
     "shell.execute_reply": "2021-11-24T13:00:14.354221Z"
    }
   },
   "outputs": [],
   "source": [
    "numNeurons_nonlinear = [302, 302, 302]\n",
    "### TO DO\n",
    "NN_deep = ---\n",
    "NN_deep.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'> Train the deep neural network </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T13:00:14.359632Z",
     "iopub.status.busy": "2021-11-24T13:00:14.359233Z",
     "iopub.status.idle": "2021-11-24T13:01:28.963907Z",
     "shell.execute_reply": "2021-11-24T13:01:28.964293Z"
    }
   },
   "outputs": [],
   "source": [
    "### TO DO\n",
    "# Define a session\n",
    "session = ---\n",
    "\n",
    "# Name the network\n",
    "nameNetwork_nonlinear = str(numNeurons_nonlinear) + \"_\" + str(numTrainingSims)\n",
    "\n",
    "### TO DO\n",
    "# Define the callbacks\n",
    "csvlogger= ---\n",
    "cp       = ---\n",
    "\n",
    "### TO DO\n",
    "# Train the network\n",
    "ae = ---\n",
    "\n",
    "### TO DO\n",
    "# Close the session\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3/3: Load trained networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize loss functions\n",
    "\n",
    "Here, we visualize the evolution of the training and cross-validation losses during the training of the linear and non-linear neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T13:01:28.971096Z",
     "iopub.status.busy": "2021-11-24T13:01:28.970659Z",
     "iopub.status.idle": "2021-11-24T13:01:30.623276Z",
     "shell.execute_reply": "2021-11-24T13:01:30.622946Z"
    }
   },
   "outputs": [],
   "source": [
    "l = {}\n",
    "l_cv = {}\n",
    "for network in [nameNetwork_linear, nameNetwork_nonlinear]:\n",
    "    l_tmp = []\n",
    "    l_cv_tmp = []\n",
    "    f = open(pathSave + \"TrainedNetworks/\" + network + \".txt\", \"r\")\n",
    "    lines = f.readlines()   \n",
    "    for _l in lines[1:]:\n",
    "        l_tmp.append(float(_l.split(',')[1]))\n",
    "        l_cv_tmp.append(float(_l.split(',')[3]))\n",
    "\n",
    "    fig = plt.figure(figsize=(8,4), dpi=160)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(l_tmp   , '-' ,label=\"Training\")\n",
    "    ax.plot(l_cv_tmp, '--',label=\"Cross-validation\") \n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"MSE\")\n",
    "    ax.set_title(network)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    l[network] = l_tmp\n",
    "    l_cv[network] = l_cv_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'> Visualize and compare the predictions of the two networks </span>\n",
    "\n",
    "Let's use the trained networks to predict temperature profiles in the test-set.\n",
    "\n",
    "First, we need to load the trained model using *tf.keras.models.load_model*(path to saved network) <br>\n",
    "Then, we use the loaded network to make predictions on the test-set using *y = model.predict(x)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T13:01:30.629095Z",
     "iopub.status.busy": "2021-11-24T13:01:30.628737Z",
     "iopub.status.idle": "2021-11-24T13:01:32.160046Z",
     "shell.execute_reply": "2021-11-24T13:01:32.160400Z"
    }
   },
   "outputs": [],
   "source": [
    "for network in [nameNetwork_linear, nameNetwork_nonlinear]:\n",
    "    print(network)\n",
    "    \n",
    "    ### TO DO\n",
    "    # load the trained model\n",
    "    NN = ---(pathSave + \"TrainedNetworks/\" + network + \".hdf5\")\n",
    "    \n",
    "    ### TO DO\n",
    "    # use the loaded model to predict\n",
    "    y_test_ML = ---\n",
    "\n",
    "    print(\"MSE on train set: \" + str(l[network][-1]))  \n",
    "    print(\"MSE on CV set: \"    + str(l_cv[network][-1]))  \n",
    "    print(\"MSE on test set: \"  + str(np.mean(np.power(y_test-y_test_ML,2))))        \n",
    "\n",
    "    # Plot the temperature profiles from simulations vs. the networks\n",
    "    profile_indices = [120, 1110, 500]\n",
    "    fig = plt.figure(figsize=(12,4), dpi=320)\n",
    "    rProf = np.linspace(0.33,1.33,y_test.shape[1])\n",
    "    pltCount = 1\n",
    "    for profIndex in profile_indices:\n",
    "        ax = fig.add_subplot(1,3,pltCount)\n",
    "        ax.plot(y_test[profIndex,:]   , rProf, \"b-\",  label=\"simulation\")\n",
    "        ax.plot(y_test_ML[profIndex,:], rProf, \"r--\", label=\"neural network\")\n",
    "        ax.set_xlabel(\"Temperature\")\n",
    "        ax.set_ylabel(\"Radius\")\n",
    "        pltCount += 1\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epilogue\n",
    "\n",
    "\n",
    "### Some other things to try\n",
    "\n",
    "- Decrease the dimnesionality of the 1D temperature profiles from 302 to 30(?) using autoencoders.\n",
    "- Try some regularization schemes like \"L2\" or \"dropout\", especially when not enough simulations are available\n",
    "- Vary hyperparameters such as architctures, learning rates, batch sizes, activation functions\n",
    "- Train longer (more epochs)\n",
    "\n",
    "### Expanding to other tasks in mantle convection\n",
    "\n",
    "<img src=\"Diagrams-Concept.svg\" alt=\"Drawing\" style=\"width: 720px;\"/>\n",
    "\n",
    "\n",
    "### Some (interesting) references\n",
    "\n",
    "**For this tutorial**\n",
    "- Agarwal, S. et al. A machine-learning-based surrogate model of Mars’ thermal evolution. Geo-physical Journal International (2020).\n",
    "\n",
    "**Numerical code (GAIA)**\n",
    "- Hüttig, C., Tosi, N. & Moore, W. An improved formulation of the incompressible navier-stokes equations with variable viscosity. Phys. Earth Planet. Inter. 220, 11–18 (2013).\n",
    "\n",
    "**Reviews**\n",
    "- Morra, G., Yuen, D. A., Tufo, H. M. & Knepley, M. G. Fresh Outlook in Numerical Methods for Geodynamics – Part 2: Big Data, HPC, Education. In Alderton, D. & Elias, S. A. (eds.) Encyclopedia of Geology, 841–855 (Academic Press, 2020).\n",
    "- Brunton, S. L., Noack, B. R. & Koumoutsakos, P. Machine Learning for Fluid Mechanics. Annual Review of Fluid Mechanics 52:1, 477-508 (2020).\n",
    "\n",
    "**Other forward modeling examples**\n",
    "- Agarwal, S., Tosi, N., Kessel, P., Breuer, D. & Montavon, G. Deep learning for surrogate modeling of two-dimensional mantle convection. Phys. Rev. Fluids 6, 113801 (2021).\n",
    "- Nonnenmacher, M. & Greenberg, D. S. Deep emulators for differentiation, forecasting, and parametrization in earth science simulators. Journal of Advances in Modeling Earth Systems 13, e2021MS002554 (2021).\n",
    "- Wandel, N., Weinmann, M. & Klein, R. Learning incompressible fluid dynamics from scratch-towards fast, differentiable fluid models that generalize. In International Conference on Learning Representations (2020).\n",
    "- Mohan, A. T., Tretiak, D., Chertkov, M. & Livescu, D. Spatio-temporal deep learning models of 3d turbulence with physics informed diagnostics. Journal of Turbulence 21, 484–524 (2020).\n",
    "- Mohan, A. T., Lubbers, N., Livescu, D. & Chertkov, M. Embedding hard physical constraints in convolutional neural networks for 3d turbulence (2020).\n",
    "- Shahnas, M. H. & Pysklywec, R. N. Toward a unified model for the thermal state of the planetary mantle – estimations from mean field deep learning. Earth and Space Science, e2019EA000881 (2020).\n",
    " \n",
    "**Inverse modeling examples**\n",
    "- Agarwal, S. et al. Toward constraining mars’ thermal evolution using machine learning. Earth and Space Science 8, e2020EA001484 (2021).\n",
    "- Magali, J.K., Bodin, T., Hedjazian, N., Samuel, H. & Atkins, S. Geodynamictomography:constraining upper-mantle deformation patterns from Bayesian inversion of surface waves. Geophysical Journal International 224, 2077–2099 (2020).\n",
    "- Atkins, S., Valentine, A. P., Tackley, P. J. & Trampert, J. Using pattern recognition to infer parameters governing mantle convection. Physics of the Earth and Planetary Interiors 257, 171 –186 (2016)\n",
    "- de Wit, R. W. L., Valentine, A. P. & Trampert, J. Bayesian inference of Earth’s radial seismic structure from body-wave traveltimes using neural networks. Geophys. Journal International 195, 408–422 (2013).\n",
    "- Meier, U., Curtis, A. & Trampert, J. Global crustal thickness from neural network inversion of surface wave data. Geophysical Journal International 169, 706–722 (2007). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
